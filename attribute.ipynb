{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Arial', sans-serif; font-size: 16px; color: #5D5C61; padding: 15px; border-radius: 10px;\">\n",
    "    <h3 style=\"color: #7395AE;\">Helper Function to load images</h3>\n",
    "    <p style=\"font-style: italic; color: #B1A296;\">This function processes and loads images for model input</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "image_path = 'Test_Fake/00A0WLZE5X.jpg'\n",
    "\n",
    "def get_image(path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(256),  \n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.float()),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet statistics\n",
    "    ])\n",
    "\n",
    "    img = cv2.imread(path)\n",
    "\n",
    "    img_tensor = transform(img)\n",
    "\n",
    "    img_tensor = img_tensor.unsqueeze(0)\n",
    "\n",
    "    return img_tensor.to(device)\n",
    "\n",
    "img_tnsr = get_image(image_path)\n",
    "print(img_tnsr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Arial', sans-serif; font-size: 16px; padding: 15px;\">\n",
    "    <h3>Visualizing Convolutional Layer Activations</h3>\n",
    "    <p>This code visualizes the activations of convolutional layers in a pre-trained ResNet-34 model when processing both real and fake images.</p>\n",
    "</div>\n",
    "<div style=\"font-family: 'Arial', sans-serif; font-size: 16px; padding: 15px;\">\n",
    "<h3>Key Steps:</h3>\n",
    "<ol>\n",
    "    <li><strong>Load the Model</strong>: A pre-trained ResNet-34 model is loaded.</li>\n",
    "    <li><strong>Extract Convolutional Layers</strong>: The code iterates through the model's structure to identify and collect all convolutional layers.</li>\n",
    "    <li><strong>Process Images</strong>: The code processes both a real and a fake image using the <code>get_image</code> function (defined above).</li>\n",
    "    <li><strong>Generate Activations</strong>: For each image:\n",
    "        <ul>\n",
    "            <li>The image is passed through each convolutional layer sequentially.</li>\n",
    "            <li>The output (activation) of each layer is collected.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>Visualize Feature Maps</strong>: For each layer's activation:\n",
    "        <ul>\n",
    "            <li>The code creates a grid of subplots.</li>\n",
    "            <li>Each subplot displays one feature map from the layer's output.</li>\n",
    "            <li>The feature maps are displayed using a grayscale colormap.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ol>\n",
    "</div>\n",
    "<div style=\"font-family: 'Arial', sans-serif; font-size: 16px; padding: 15px;\">\n",
    "<h3>Purpose:</h3>\n",
    "<p style=\"font-style: italic;\">This visualization helps understand how the convolutional layers in the ResNet-34 model process and transform the input image at different stages. By comparing the activations for real and fake images, we can potentially identify patterns that the model uses to distinguish between them.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('mps')\n",
    "\n",
    "model = torch.load('models/resnet_34.pkl').to(device)\n",
    "\n",
    "model_children = list(model.children())\n",
    "num_layers = 0\n",
    "conv_layers = []\n",
    "for child in model_children:\n",
    "    for model in child.children():\n",
    "        for layer in model.children():\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                conv_layers.append(layer)\n",
    "                num_layers += 1\n",
    "            elif isinstance(layer, nn.Sequential):\n",
    "                for sub in layer.children():\n",
    "                    for convs in sub.children():\n",
    "                        if isinstance(convs, nn.Conv2d):\n",
    "                            conv_layers.append(convs)\n",
    "                            num_layers += 1\n",
    "\n",
    "image_path = 'Test_Fake/00A0WLZE5X.jpg'\n",
    "img_tnsr = get_image(image_path)\n",
    "\n",
    "results = [conv_layers[0](img_tnsr)]\n",
    "for i in range(1, len(conv_layers)):\n",
    "    results.append(conv_layers[i](results[-1]))\n",
    "\n",
    "for i in range(len(results)):\n",
    "    layer_viz = results[i].squeeze()\n",
    "    num_feature_maps = layer_viz.size(0)\n",
    "\n",
    "    num_cols = 8\n",
    "    num_rows = (num_feature_maps + num_cols - 1) // num_cols\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 4 * num_rows))\n",
    "    axes = axes.flatten()\n",
    "    print(f'Layer: {i + 1}')\n",
    "    for j, f in enumerate(layer_viz):\n",
    "        if j < len(axes):\n",
    "            axes[j].imshow(f.detach().cpu().numpy(), cmap='gray')\n",
    "            axes[j].axis('off')\n",
    "        else:\n",
    "            break\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import visualization as viz\n",
    "import torch\n",
    "\n",
    "device = torch.device('mps')\n",
    "model = torch.load('models/resnet_34.pkl').to(device)\n",
    "\n",
    "# Real\n",
    "folder_path_real = \"Test_Real\"\n",
    "real_images = [folder_path_real + '/'+ f for f in os.listdir(folder_path_real) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "folder_path_fake = \"Test_Fake\"\n",
    "fake_images = [folder_path_fake + '/' + f for f in os.listdir(folder_path_fake) if f.endswith(('.jpg', '.jpeg', '.png'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "image_path = 'deepfake dataset/Real/00022.jpg'\n",
    "\n",
    "def get_image(path):\n",
    "    # Define the transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(256),  # Resize the image to 256x256\n",
    "        transforms.CenterCrop(224),  # Crop the center 224x224 portion\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.float()),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet statistics\n",
    "    ])\n",
    "\n",
    "    # Load the image\n",
    "    img = cv2.imread(path)\n",
    "\n",
    "    # Apply the transformations\n",
    "    img_tensor = transform(img)\n",
    "\n",
    "    # Add a batch dimension\n",
    "    img_tensor = img_tensor.unsqueeze(0)\n",
    "\n",
    "    return img_tensor.to(device)  # Move the tensor to the desired device\n",
    "\n",
    "# Get the preprocessed image tensor\n",
    "img_tnsr = get_image(image_path)\n",
    "print(img_tnsr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrated Gradients\n",
    "- Integrated Gradients is an attribution method used to explain the predictions of deep learning models. It assigns importance scores to each input feature (in this case, pixels of an image) based on how much they contribute to the model's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = IntegratedGradients(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "target_class = 0\n",
    "\n",
    "if target_class == 0:\n",
    "    for i,image_file in enumerate(fake_images):\n",
    "\n",
    "        img_tnsr = get_image(image_file)\n",
    "        \n",
    "        baseline = torch.zeros_like(img_tnsr)\n",
    "        \n",
    "        attributions = ig.attribute(img_tnsr, baselines=baseline, target=target_class, n_steps=100)\n",
    "        \n",
    "        vis_img = viz.visualize_image_attr(\n",
    "            attributions.squeeze().cpu().detach().numpy().transpose(1, 2, 0),\n",
    "            img_tnsr.squeeze().cpu().detach().numpy().transpose(1, 2, 0),\n",
    "            method='blended_heat_map',\n",
    "            sign='positive',\n",
    "            show_colorbar=True,\n",
    "            cmap='inferno',\n",
    "            title=f'Integrated Gradients for {image_file}',\n",
    "            fig_size=(10, 10)\n",
    "        )\n",
    "        \n",
    "        vis_img[0].savefig(f'ig_{i+1}_fake',bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "elif target_class == 1:\n",
    "    for i,image_file in enumerate(real_images):\n",
    "        img_tnsr = get_image(image_file)\n",
    "        \n",
    "        baseline = torch.zeros_like(img_tnsr)\n",
    "        \n",
    "        attributions = ig.attribute(img_tnsr, baselines=baseline, target=target_class, n_steps=100)\n",
    "        \n",
    "        vis_img = viz.visualize_image_attr(\n",
    "            attributions.squeeze().cpu().detach().numpy().transpose(1, 2, 0),\n",
    "            img_tnsr.squeeze().cpu().detach().numpy().transpose(1, 2, 0),\n",
    "            method='blended_heat_map',\n",
    "            sign='positive',\n",
    "            show_colorbar=True,\n",
    "            cmap='inferno',\n",
    "            title=f'Integrated Gradients for {image_file}',\n",
    "            fig_size=(10, 10)\n",
    "        )\n",
    "        \n",
    "        vis_img[0].savefig(f'ig_{i+1}_real',bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saliency\n",
    "- The og technique, returns the gradients with respect to the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import Saliency\n",
    "\n",
    "sal = Saliency(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes = [0,1]\n",
    "for target_class in target_classes:\n",
    "    if target_class == 0:\n",
    "        for i,image_file in enumerate(fake_images):\n",
    "\n",
    "            img_tnsr = get_image(image_file)\n",
    "            \n",
    "            attributions = sal.attribute(img_tnsr, target=target_class)\n",
    "            \n",
    "            vis_img = viz.visualize_image_attr(\n",
    "                attributions.squeeze().cpu().detach().numpy().transpose(1, 2, 0),\n",
    "                img_tnsr.squeeze().cpu().detach().numpy().transpose(1, 2, 0),\n",
    "                method='blended_heat_map',\n",
    "                sign='positive',\n",
    "                show_colorbar=True,\n",
    "                cmap='inferno',\n",
    "                title=f'Integrated Gradients for {image_file}',\n",
    "                fig_size=(10, 10)\n",
    "            )\n",
    "            \n",
    "            vis_img[0].savefig(f'sal_{i+1}_fake',bbox_inches='tight', pad_inches=0)\n",
    "            \n",
    "    elif target_class == 1:\n",
    "        for i,image_file in enumerate(real_images):\n",
    "            img_tnsr = get_image(image_file)\n",
    "            \n",
    "            baseline = torch.zeros_like(img_tnsr)\n",
    "\n",
    "            attributions = sal.attribute(img_tnsr, target=target_class)\n",
    "            \n",
    "            vis_img = viz.visualize_image_attr(\n",
    "                attributions.squeeze().cpu().detach().numpy().transpose(1, 2, 0),\n",
    "                img_tnsr.squeeze().cpu().detach().numpy().transpose(1, 2, 0),\n",
    "                method='blended_heat_map',\n",
    "                sign='positive',\n",
    "                show_colorbar=True,\n",
    "                cmap='inferno',\n",
    "                title=f'Integrated Gradients for {image_file}',\n",
    "                fig_size=(10, 10)\n",
    "            )\n",
    "            \n",
    "            vis_img[0].savefig(f'sal_{i+1}_real',bbox_inches='tight', pad_inches=0)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepLiftSHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import DeepLiftShap\n",
    "\n",
    "dl = DeepLiftShap(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes = [0,1]\n",
    "\n",
    "for target_class in target_classes:\n",
    "    if target_class == 0:\n",
    "        for i,image_file in enumerate(fake_images):\n",
    "\n",
    "            img_tnsr = get_image(image_file)\n",
    "            \n",
    "            baselines = torch.rand(20,3,224,224)\n",
    "            \n",
    "            attributions = dl.attribute(img_tnsr, baselines=baselines ,target=target_class)\n",
    "            \n",
    "            vis_img = viz.visualize_image_attr(\n",
    "                attributions.squeeze().cpu().detach().numpy().transpose(1, 2, 0),\n",
    "                img_tnsr.squeeze().cpu().detach().numpy().transpose(1, 2, 0),\n",
    "                method='blended_heat_map',\n",
    "                sign='positive',\n",
    "                show_colorbar=True,\n",
    "                cmap='inferno',\n",
    "                title=f'Integrated Gradients for {image_file}',\n",
    "                fig_size=(10, 10)\n",
    "            )\n",
    "            \n",
    "            vis_img[0].savefig(f'DLSHAP_{i+1}_fake',bbox_inches='tight', pad_inches=0)\n",
    "    elif target_class == 1:\n",
    "        for i,image_file in enumerate(real_images):\n",
    "\n",
    "            img_tnsr = get_image(image_file)\n",
    "            \n",
    "            baselines = torch.rand(20,3,224,224)\n",
    "            \n",
    "            attributions = dl.attribute(img_tnsr, baselines=baselines ,target=target_class)\n",
    "            \n",
    "            vis_img = viz.visualize_image_attr(\n",
    "                attributions.squeeze().cpu().detach().numpy().transpose(1, 2, 0),\n",
    "                img_tnsr.squeeze().cpu().detach().numpy().transpose(1, 2, 0),\n",
    "                method='blended_heat_map',\n",
    "                sign='positive',\n",
    "                show_colorbar=True,\n",
    "                cmap='inferno',\n",
    "                title=f'Integrated Gradients for {image_file}',\n",
    "                fig_size=(10, 10)\n",
    "            )\n",
    "            \n",
    "            vis_img[0].savefig(f'DLSHAP_{i+1}_real',bbox_inches='tight', pad_inches=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input X Gradient\n",
    "- A baseline approach for computing the attribution. It multiplies input with the gradient with respect to input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import InputXGradient\n",
    "\n",
    "ixg = InputXGradient(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes = [0,1]\n",
    "\n",
    "for target_class in target_classes:\n",
    "    if target_class == 0:\n",
    "        for i,image_file in enumerate(fake_images):\n",
    "\n",
    "            img_tnsr = get_image(image_file)\n",
    "            \n",
    "            attributions = ixg.attribute(img_tnsr ,target=target_class)\n",
    "            \n",
    "            vis_img = viz.visualize_image_attr(\n",
    "                attributions.squeeze().cpu().detach().numpy().transpose(1, 2, 0),\n",
    "                img_tnsr.squeeze().cpu().detach().numpy().transpose(1, 2, 0),\n",
    "                method='blended_heat_map',\n",
    "                sign='positive',\n",
    "                show_colorbar=True,\n",
    "                cmap='inferno',\n",
    "                title=f'Integrated Gradients for {image_file}',\n",
    "                fig_size=(10, 10)\n",
    "            )\n",
    "            \n",
    "            vis_img[0].savefig(f'IXG_{i+1}_fake',bbox_inches='tight', pad_inches=0)\n",
    "    elif target_class == 1:\n",
    "        for i,image_file in enumerate(real_images):\n",
    "\n",
    "            img_tnsr = get_image(image_file)\n",
    "                        \n",
    "            attributions = ixg.attribute(img_tnsr,target=target_class)\n",
    "            \n",
    "            vis_img = viz.visualize_image_attr(\n",
    "                attributions.squeeze().cpu().detach().numpy().transpose(1, 2, 0),\n",
    "                img_tnsr.squeeze().cpu().detach().numpy().transpose(1, 2, 0),\n",
    "                method='blended_heat_map',\n",
    "                sign='positive',\n",
    "                show_colorbar=True,\n",
    "                cmap='inferno',\n",
    "                title=f'Integrated Gradients for {image_file}',\n",
    "                fig_size=(10, 10)\n",
    "            )\n",
    "            \n",
    "            vis_img[0].savefig(f'IXG_{i+1}_real',bbox_inches='tight', pad_inches=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Ablation\n",
    "- A perturbation based approach to computing attribution, involving replacing each input feature with a given baseline / reference, and computing the difference in output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import FeatureAblation\n",
    "device = torch.device('mps')\n",
    "model.to(device)\n",
    "featabl = FeatureAblation(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes = [0,1]\n",
    "\n",
    "for target_class in target_classes:\n",
    "    if target_class == 0:\n",
    "        for i,image_file in enumerate(fake_images[:3]):\n",
    "\n",
    "            img_tnsr = get_image(image_file).to(device)\n",
    "\n",
    "            attributions = featabl.attribute(img_tnsr ,target = target_class,show_progress = True,perturbations_per_eval = 25)\n",
    "            \n",
    "            vis_img = viz.visualize_image_attr(\n",
    "                attributions.squeeze().cpu().detach().numpy().transpose(1, 2, 0),\n",
    "                img_tnsr.squeeze().cpu().detach().numpy().transpose(1, 2, 0),\n",
    "                method='blended_heat_map',\n",
    "                sign='positive',\n",
    "                show_colorbar=True,\n",
    "                cmap='inferno',\n",
    "                title=f'Integrated Gradients for {image_file}',\n",
    "                fig_size=(10, 10)\n",
    "            )\n",
    "            \n",
    "            vis_img[0].savefig(f'featabl_{i+1}_fake',bbox_inches='tight', pad_inches=0)\n",
    "    elif target_class == 1:\n",
    "        for i,image_file in enumerate(real_images[:3]):\n",
    "\n",
    "            img_tnsr = get_image(image_file).to(device)\n",
    "                        \n",
    "            attributions = featabl.attribute(img_tnsr,target = target_class,show_progress = True,perturbations_per_eval = 25)\n",
    "            \n",
    "            vis_img = viz.visualize_image_attr(\n",
    "                attributions.squeeze().cpu().detach().numpy().transpose(1, 2, 0),\n",
    "                img_tnsr.squeeze().cpu().detach().numpy().transpose(1, 2, 0),\n",
    "                method='blended_heat_map',\n",
    "                sign='positive',\n",
    "                show_colorbar=True,\n",
    "                cmap='inferno',\n",
    "                title=f'Integrated Gradients for {image_file}',\n",
    "                fig_size=(10, 10)\n",
    "            )\n",
    "            \n",
    "            vis_img[0].savefig(f'featabl_{i+1}_real',bbox_inches='tight', pad_inches=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
